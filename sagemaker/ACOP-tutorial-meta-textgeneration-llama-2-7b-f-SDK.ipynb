{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8af3794b",
   "metadata": {},
   "source": [
    "# Run Llama 2 Models in SageMaker JumpStart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446b1b24",
   "metadata": {},
   "source": [
    "---\n",
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK to deploy a JumpStart model for Text Generation using the Llama 2 fine-tuned model optimized for dialogue use cases.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35642ab2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32b2da2e-cb9c-44a8-9fd4-759e13ac459c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-25 00:59:09.390737\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b55e677-3429-4668-b100-bd63d2a4c401",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d458cf0-02e2-4066-927b-25fa5ef2a07e",
   "metadata": {},
   "source": [
    "***\n",
    "You can continue with the default model or choose a different model: this notebook will run with the following model IDs :\n",
    "- `meta-textgeneration-llama-2-7b-f`\n",
    "- `meta-textgeneration-llama-2-13b-f`\n",
    "- `meta-textgeneration-llama-2-70b-f`\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62fd54f6",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdOnly"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-textgeneration-llama-2-7b-f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "084aa53c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_version = \"3.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eef0dd",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "\n",
    "***\n",
    "You can now deploy the model using SageMaker JumpStart. For successful deployment, you must manually change the `accept_eula` argument in the model's deploy method to `True`.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e52afae-868d-4736-881f-7180f393003a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using vulnerable JumpStart model 'meta-textgeneration-llama-2-7b-f' and version '3.2.0'.\n",
      "Using model 'meta-textgeneration-llama-2-7b-f' with wildcard version identifier '3.*'. You can pin to version '3.2.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.69 s, sys: 4.6 s, total: 6.29 s\n",
      "Wall time: 1.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "model = JumpStartModel(model_id=model_id, model_version=model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a48f26bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!CPU times: user 65.4 ms, sys: 15.4 ms, total: 80.8 ms\n",
      "Wall time: 6min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictor = model.deploy(accept_eula=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef7207e-01ba-4ac2-b4a9-c8f6f0e1c498",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invoke the endpoint\n",
    "\n",
    "***\n",
    "### Supported Parameters\n",
    "\n",
    "***\n",
    "This model supports many parameters while performing inference. They include:\n",
    "\n",
    "* **max_length:** Model generates text until the output length (which includes the input context length) reaches `max_length`. If specified, it must be a positive integer.\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches `max_new_tokens`. If specified, it must be a positive integer.\n",
    "* **num_beams:** Number of beams used in the greedy search. If specified, it must be integer greater than or equal to `num_return_sequences`.\n",
    "* **no_repeat_ngram_size:** Model ensures that a sequence of words of `no_repeat_ngram_size` is not repeated in the output sequence. If specified, it must be a positive integer greater than 1.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **early_stopping:** If True, text generation is finished when all beam hypotheses reach the end of sentence token. If specified, it must be boolean.\n",
    "* **do_sample:** If True, sample the next word as per the likelihood. If specified, it must be boolean.\n",
    "* **top_k:** In each step of text generation, sample from only the `top_k` most likely words. If specified, it must be a positive integer.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **return_full_text:** If True, input text will be part of the output generated text. If specified, it must be boolean. The default value for it is False.\n",
    "* **stop**: If specified, it must be a list of strings. Text generation stops if any one of the specified strings is generated.\n",
    "\n",
    "We may specify any subset of the parameters mentioned above while invoking an endpoint. Next, we show an example of how to invoke endpoint with these arguments.\n",
    "\n",
    "**NOTE**: If `max_new_tokens` is not defined, the model may generate up to the maximum total tokens allowed, which is 4K for these models. This may result in endpoint query timeout errors, so it is recommended to set `max_new_tokens` when possible. For 7B, 13B, and 70B models, we recommend to set `max_new_tokens` no greater than 1500, 1000, and 500 respectively, while keeping the total number of tokens less than 4K.\n",
    "\n",
    "**NOTE**: This model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and alternating (u/a/u/a/u...).\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e845ae",
   "metadata": {},
   "source": [
    "### Example prompts\n",
    "***\n",
    "The examples in this section demonstrate how to perform text generation with conversational dialog as prompt inputs. Example payloads are retrieved programmatically from the `JumpStartModel` object.\n",
    "\n",
    "Input messages for Llama-2 chat models should exhibit the following format. The model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and alternating (u/a/u/a/u...). The last message must be from 'user'. A simple user prompt may look like the following:\n",
    "```\n",
    "<s>[INST] {user_prompt} [/INST]\n",
    "```\n",
    "You may also add a system prompt with the following syntax:\n",
    "```\n",
    "<s>[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "\n",
    "{user_prompt} [/INST]\n",
    "```\n",
    "Finally, you can have a conversational interaction with the model by including all previous user prompts and assistant responses in the input:\n",
    "```\n",
    "<s>[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "\n",
    "{user_prompt_1} [/INST] {assistant_response_1} </s><s>[INST] {user_prompt_1} [/INST]\n",
    "```\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2255b012-0f95-427c-bf1f-9104f651b5e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 292 ¬µs, sys: 0 ns, total: 292 ¬µs\n",
      "Wall time: 297 ¬µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "example_payloads = model.retrieve_all_examples()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "952395d9-2254-4fe6-b42c-f4f763c93744",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': '<s>[INST] what is the recipe of mayonnaise? [/INST] ',\n",
       " 'parameters': {'max_new_tokens': 256,\n",
       "  'top_p': 0.9,\n",
       "  'temperature': 0.6,\n",
       "  'decoder_input_details': True,\n",
       "  'details': True}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_payloads[0].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fc07501",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input\n",
      " {'inputs': '<s>[INST] what is the recipe of mayonnaise? [/INST] ', 'parameters': {'max_new_tokens': 256, 'top_p': 0.9, 'temperature': 0.6, 'decoder_input_details': True, 'details': True}} \n",
      "\n",
      "Output\n",
      " Mayonnaise is a thick, creamy condiment made from a mixture of egg yolks, oil, and an acid, such as vinegar or lemon juice. Here is a basic recipe for homemade mayonnaise:\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 3 large egg yolks\n",
      "* 1/2 cup (120 ml) neutral-tasting oil, such as canola or grapeseed\n",
      "* 1 tablespoon (15 ml) vinegar or lemon juice\n",
      "* Salt and pepper to taste\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. In a medium-sized bowl, whisk together the egg yolks and vinegar or lemon juice until the mixture becomes thick and pale yellow in color.\n",
      "2. Slowly pour in the oil while continuously whisking the mixture. The mixture should thicken and emulsify as you add the oil.\n",
      "3. Continue whisking until the mixture is smooth and creamy, and has a thick, velvety texture. This should take about 5-7 minutes.\n",
      "4. Taste and adjust the seasoning \n",
      "\n",
      "===============\n",
      "\n",
      "Input\n",
      " {'inputs': \"<s>[INST] I am going to Paris, what should I see? [/INST] Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\\n\\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\\n\\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.</s><s>[INST] What is so great about #1? [/INST] \", 'parameters': {'max_new_tokens': 256, 'top_p': 0.9, 'temperature': 0.6}} \n",
      "\n",
      "Output\n",
      " The Eiffel Tower is considered one of the most iconic landmarks in Paris and one of the most recognizable structures in the world. Here are some of the reasons why it's so great:\n",
      "\n",
      "1. Unique Design: The Eiffel Tower's unique design, with its lattice-like structure and intricate details, makes it a standout among other landmarks. Its innovative design was revolutionary for its time and has become an iconic symbol of Paris and France.\n",
      "2. Breathtaking Views: The Eiffel Tower offers stunning views of the city of Paris and its surroundings. From the top, you can see the entire city, including the Seine River, the Arc de Triomphe, and the Champs-√âlys√©es.\n",
      "3. Historical Significance: The Eiffel Tower was built for the 1889 World's Fair, held in Paris, and was originally intended to be a temporary structure. However, it became an instant hit and was left standing after the fair, becoming a permanent fixture in the city's skyline.\n",
      "4. Cultural Impact: The Eiffel Tower has had a significant impact on popular culture, \n",
      "\n",
      "===============\n",
      "\n",
      "Input\n",
      " {'inputs': '<s>[INST] <<SYS>>\\nAlways answer with Haiku\\n<</SYS>>\\n\\nI am going to Paris, what should I see? [/INST] ', 'parameters': {'max_new_tokens': 256, 'top_p': 0.9, 'temperature': 0.6}} \n",
      "\n",
      "Output\n",
      " Eiffel Tower high\n",
      "Love locks on bridges glow\n",
      "City of romance \n",
      "\n",
      "===============\n",
      "\n",
      "Input\n",
      " {'inputs': '<s>[INST] <<SYS>>\\nAlways answer with emojis\\n<</SYS>>\\n\\nHow to go from Beijing to NY? [/INST] ', 'parameters': {'max_new_tokens': 256, 'top_p': 0.9, 'temperature': 0.6}} \n",
      "\n",
      "Output\n",
      " Here's how to go from Beijing to NY üõ´üó∫Ô∏è:\n",
      "\n",
      "1. Fly from Beijing Capital International Airport (PEK) to John F. Kennedy International Airport (JFK) or Newark Liberty International Airport (EWR) in New York City. üöÄ\n",
      "2. Take a taxi or public transportation from the airport to your destination in New York City. üöïüí®\n",
      "\n",
      "Total distance: approximately 8,000 miles (13,000 km) üìà\n",
      "\n",
      "Travel time: around 12-14 hours by flight, depending on layovers and traffic. ‚è±Ô∏è\n",
      "\n",
      "Have a great trip! üòä \n",
      "\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for payload in example_payloads:\n",
    "    response = predictor.predict(payload.body)\n",
    "    print(\"\\nInput\\n\", payload.body, \"\\n\\nOutput\\n\", response[0][\"generated_text\"], \"\\n\\n===============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d76d95",
   "metadata": {},
   "source": [
    "***\n",
    "While not used in the previously provided example payloads, you can format your own messages to the Llama-2 model with the following utility function.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5adf9b4-c7e1-4090-aefe-9cae0d096968",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] <<SYS>>\\nYou are writing strategic objective statements for US government goals. You work for the US Department of Health and Human Services. Each statement should clearly communicate what the agency is trying to accomplish. It should energize Americans who have a stake in the issue to get involved. Rewrite the strategic objective statement at the bottom to make it stronger. Generate three options. Do not use jargon. Write for a 10th-grade reading level or lower. Format the statement so that it is easy to skim and read. Use less than 225 characters.\\n<</SYS>>\\n\\nRewrite: Drive the integration of behavioral health into the healthcare system to strengthen and expand access to mental health and substance use disorder treatment and recovery services for individuals and families. [/INST] '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def format_messages(messages: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"Format messages for Llama-2 chat models.\n",
    "    \n",
    "    The model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and \n",
    "    alternating (u/a/u/a/u...). The last message must be from 'user'.\n",
    "    \"\"\"\n",
    "    prompt: List[str] = []\n",
    "\n",
    "    if messages[0][\"role\"] == \"system\":\n",
    "        content = \"\".join([\"<<SYS>>\\n\", messages[0][\"content\"], \"\\n<</SYS>>\\n\\n\", messages[1][\"content\"]])\n",
    "        messages = [{\"role\": messages[1][\"role\"], \"content\": content}] + messages[2:]\n",
    "\n",
    "    for user, answer in zip(messages[::2], messages[1::2]):\n",
    "        prompt.extend([\"<s>\", \"[INST] \", (user[\"content\"]).strip(), \" [/INST] \", (answer[\"content\"]).strip(), \"</s>\"])\n",
    "\n",
    "    prompt.extend([\"<s>\", \"[INST] \", (messages[-1][\"content\"]).strip(), \" [/INST] \"])\n",
    "\n",
    "    return \"\".join(prompt)\n",
    "\n",
    "\n",
    "dialog = [\n",
    "    {\"role\": \"system\", \"content\": \"You are writing strategic objective statements for US government goals. You work for the US Department of Health and Human Services. Each statement should clearly communicate what the agency is trying to accomplish. It should energize Americans who have a stake in the issue to get involved. Rewrite the strategic objective statement at the bottom to make it stronger. Generate three options. Do not use jargon. Write for a 10th-grade reading level or lower. Format the statement so that it is easy to skim and read. Use less than 225 characters.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Rewrite: Drive the integration of behavioral health into the healthcare system to strengthen and expand access to mental health and substance use disorder treatment and recovery services for individuals and families.\"\"\"},\n",
    "]\n",
    "\n",
    "prompt = format_messages(dialog)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a705dc5-3f1e-4764-a489-f0a9f5ff1d91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload= {'inputs': prompt,\n",
    " 'parameters': {'max_new_tokens': 256,\n",
    "  'top_p': 0.9,\n",
    "  'temperature': 0.6,\n",
    "  'decoder_input_details': True,\n",
    "  'details': True}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af602a42-79f9-4ae7-9247-a6a2d61f767f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option 1:\n",
      "\"Unlock Access to Mental Health & Substance Use Treatment: Integrate Behavioral Health into Healthcare to Strengthen Care for All.\"\n",
      "\n",
      "Option 2:\n",
      "\"Transform Healthcare with Behavioral Health Integration: Ensure Equitable Access to Mental Health & Substance Use Treatment for All.\"\n",
      "\n",
      "Option 3:\n",
      "\"Healthy Minds, Healthy Lives: Integrate Behavioral Health into Healthcare to Expand Access to Mental Health & Substance Use Treatment & Recovery Services.\"\n",
      "\n",
      "Each of these options is clear, concise, and easy to understand, with a focus on the importance of integrating behavioral health into the healthcare system to improve access to mental health and substance use disorder treatment and recovery services for individuals and families. They also use language that is relatable and motivating, and avoid technical jargon to make them accessible to a wide range of audiences.\n"
     ]
    }
   ],
   "source": [
    "response = predictor.predict(payload)\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62e23428-4dda-4b31-a160-aa602c4816db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reformat_user_prompt(user_prompt):\n",
    "    return f\"<s>[INST] {user_prompt} [/INST]\"\n",
    "\n",
    "additional_examples = [\n",
    "    {\n",
    "    \"inputs\": \"Tell me about General Services Administration.\",\n",
    "    \"parameters\": {\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.8,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"stop\": [\"<|endoftext|>\", \"</s>\"]\n",
    "    }\n",
    "},  # tell me about...\n",
    "{\n",
    "    \"inputs\": \"Tell me about Amazon SageMaker.\",\n",
    "    \"parameters\": {\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.8,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"stop\": [\"<|endoftext|>\", \"</s>\"]\n",
    "    }\n",
    "}, # summarization\n",
    " {\n",
    "    \"inputs\":\"\"\"Starting today, the state-of-the-art Falcon 40B foundation model from Technology\n",
    "    Innovation Institute (TII) is available on Amazon SageMaker JumpStart, SageMaker's machine learning (ML) hub\n",
    "    that offers pre-trained models, built-in algorithms, and pre-built solution templates to help you quickly get\n",
    "    started with ML. You can deploy and use this Falcon LLM with a few clicks in SageMaker Studio or\n",
    "    programmatically through the SageMaker Python SDK.\n",
    "    Falcon 40B is a 40-billion-parameter large language model (LLM) available under the Apache 2.0 license that\n",
    "    ranked #1 in Hugging Face Open LLM leaderboard, which tracks, ranks, and evaluates LLMs across multiple\n",
    "    benchmarks to identify top performing models. Since its release in May 2023, Falcon 40B has demonstrated\n",
    "    exceptional performance without specialized fine-tuning. To make it easier for customers to access this\n",
    "    state-of-the-art model, AWS has made Falcon 40B available to customers via Amazon SageMaker JumpStart.\n",
    "    Now customers can quickly and easily deploy their own Falcon 40B model and customize it to fit their specific\n",
    "    needs for applications such as translation, question answering, and summarizing information.\n",
    "    Falcon 40B are generally available today through Amazon SageMaker JumpStart in US East (Ohio),\n",
    "    US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), Asia Pacific (Seoul), Asia Pacific (Mumbai),\n",
    "    Europe (London), Europe (Frankfurt), Europe (Ireland), and Canada (Central),\n",
    "    with availability in additional AWS Regions coming soon. To learn how to use this new feature,\n",
    "    please see SageMaker JumpStart documentation, the Introduction to SageMaker JumpStart ‚Äì\n",
    "    Text Generation with Falcon LLMs example notebook, and the blog Technology Innovation Institute trainsthe\n",
    "    state-of-the-art Falcon LLM 40B foundation model on Amazon SageMaker. Summarize the article above:\"\"\",\n",
    "    \"parameters\":{\n",
    "        \"max_new_tokens\":200\n",
    "        }\n",
    "    },\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "220dda53-a298-42a1-a951-b24cd72288fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_endpoint(payload):\n",
    "    \"\"\"Query endpoint and print the response\"\"\"\n",
    "    response = predictor.predict(payload)\n",
    "    print(f\"\\033[1m Input:\\033[0m {payload['inputs']}\")\n",
    "    print(f\"\\033[1m Output:\\033[0m {response[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b329707-ec03-4140-bb15-1ecd3c3669b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ex in additional_examples:\n",
    "    text = reformat_user_prompt(ex[\"inputs\"])\n",
    "    ex[\"inputs\"] = text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec73cbab-1336-4227-88e5-ae41f152e8dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(additional_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98383b09-b255-4ce8-98fe-9f0f2800f133",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Input:\u001b[0m <s>[INST] Tell me about General Services Administration. [/INST]\n",
      "\u001b[1m Output:\u001b[0m   The General Services Administration (GSA) is a federal agency in the United States that provides centralized support services to other federal agencies. The GSA was established in 1949 to help federal agencies acquire goods and services more efficiently and cost-effectively. The agency is responsible for managing the government's real estate, fleet, and acquisition of goods and services.\n",
      "\n",
      "The GSA has several key responsibilities:\n",
      "\n",
      "1. Real Estate Management: The GSA is responsible for managing the federal government's real estate portfolio, which includes over 9,000 buildings across the country. The agency leases and manages these buildings on behalf of federal agencies, as well as provides design and construction services for new buildings.\n",
      "2. Fleet Management: The GSA manages the federal government's fleet of vehicles, including cars, trucks, and aircraft. The agency also provides fuel cards and other fleet management services to federal agencies.\n",
      "3. Acquisition Services: The GSA provides acquisition services to federal agencies, including purchasing goods and services, managing contracts, and providing logistics support. The agency also operates the Federal Acquisition Service (FAS), which is the primary source of acquisition services for the federal government.\n",
      "4. Information Technology: The GSA provides information technology services to federal agencies, including cloud computing, cybersecurity, and data center management.\n",
      "5. Sustainability: The GSA is committed to promoting sustainability in the federal government's operations, including energy efficiency, renewable energy, and waste reduction.\n",
      "6. Governmentwide Policy: The GSA sets governmentwide policy for a variety of functions, including procurement, real estate, and fleet management.\n",
      "\n",
      "The GSA is headed by a Administrator, who is appointed by the President and confirmed by the Senate. The agency has a staff of over 10,000 employees, including professionals in fields such as real estate management, acquisition, and information technology.\n",
      "\n",
      "The GSA has a number of programs and initiatives aimed at promoting efficiency, innovation, and sustainability in the federal government's operations. These include:\n",
      "\n",
      "1. The Federal Marketplace: The GSA operates the Federal Marketplace, a platform that provides federal agencies with access to a wide range of products and services from private sector providers.\n",
      "2. The Green Proving Ground: The GSA's Green Proving Ground is a program that promotes the use of sustainable technologies in federal buildings.\n",
      "3. The Federal Acquisition Service (FAS) 2020: The FAS 2020 is a strategic plan that aims to improve the efficiency and effectiveness of the federal government's acquisition processes.\n",
      "4. The GSA Innovation Center: The GSA Innovation Center is a hub for innovation and experimentation, where the agency works with private sector partners to develop and test new ideas for improving government operations.\n",
      "\n",
      "Overall, the GSA plays a critical role in supporting the operations of the federal government by providing centralized support services that help agencies save time and money, and promote sustainability in their operations.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\u001b[1m Input:\u001b[0m <s>[INST] Tell me about Amazon SageMaker. [/INST]\n",
      "\u001b[1m Output:\u001b[0m   Amazon SageMaker is a cloud-based machine learning (ML) platform offered by Amazon Web Services (AWS). It provides a range of tools and services for data scientists and ML practitioners to build, train, and deploy ML models. Here are some key features and capabilities of Amazon SageMaker:\n",
      "\n",
      "1. Data Ingestion: Amazon SageMaker provides data ingestion tools to collect, transform, and store data from various sources, including Amazon S3, Amazon Redshift, and Amazon DynamoDB.\n",
      "2. Data Preparation: The platform offers data preparation tools to clean, preprocess, and transform data, including data transformation, feature engineering, and data augmentation.\n",
      "3. Model Building: Amazon SageMaker provides a range of algorithms and frameworks for building ML models, including deep learning, natural language processing, and statistical modeling.\n",
      "4. Model Training: The platform offers tools for training ML models, including distributed training, hyperparameter tuning, and model optimization.\n",
      "5. Model Evaluation: Amazon SageMaker provides tools for evaluating the performance of ML models, including model testing, validation, and hyperparameter tuning.\n",
      "6. Model Deployment: The platform offers tools for deploying ML models to various environments, including AWS Lambda, Amazon EC2, and Amazon Elastic Container Service (ECS).\n",
      "7. Automated Machine Learning (AutoML): Amazon SageMaker provides automated ML capabilities, including automatic feature engineering, model selection, and hyperparameter tuning.\n",
      "8. Collaboration and Security: The platform offers collaboration tools for ML teams, including version control, Git integration, and role-based access control. Amazon SageMaker also provides security features, such as data encryption and access controls, to ensure the privacy and security of data.\n",
      "9. Integration with Other AWS Services: Amazon SageMaker integrates with other AWS services, such as Amazon S3, Amazon Redshift, and Amazon QuickSight, to provide a complete ML solution.\n",
      "\n",
      "Benefits of Using Amazon SageMaker:\n",
      "\n",
      "1. Faster Time-to-Insight: Amazon SageMaker accelerates the ML process by providing pre-built algorithms, automated workflows, and high-performance computing capabilities.\n",
      "2. Increased Productivity: The platform automates many of the repetitive tasks involved in ML, such as data preparation and model training, allowing data scientists to focus on more strategic tasks.\n",
      "3. Improved Model Performance: Amazon SageMaker provides tools for evaluating and optimizing ML models, leading to better model performance and accuracy.\n",
      "4. Secure and Compliant: Amazon SageMaker provides security and compliance features, such as data encryption and access controls, to ensure the privacy and security of data.\n",
      "5. Scalability: The platform can handle large-scale ML tasks and can scale up or down based on the needs of the organization.\n",
      "\n",
      "Use Cases for Amazon SageMaker:\n",
      "\n",
      "1. Fraud Detection: Amazon SageMaker can be used to build ML models that detect fraudulent activity in financial transactions, insurance claims, or other domains.\n",
      "2. Image Recognition: The platform can be used to build ML models that recognize objects in images, classify images, or detect anomalies in image data.\n",
      "3. Natural Language Processing: Amazon SageMaker can be used to build ML models that process and analyze natural language data, such as text, voice, or sentiment analysis.\n",
      "4. Predictive Maintenance: The platform can be used to build ML models that predict equipment failures or maintenance needs based on sensor data or other sources.\n",
      "5. Recommendation Systems: Amazon SageMaker can be used to build ML models that recommend products or services based on user behavior, preferences, or other factors.\n",
      "\n",
      "In summary, Amazon SageMaker is a powerful ML platform that provides a range of tools and services for data scientists and ML practitioners to build, train, and deploy ML models. It offers automated ML capabilities, data preparation and transformation tools, and integration with other AWS services, making it a popular choice for organizations looking to leverage the power of ML.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\u001b[1m Input:\u001b[0m <s>[INST] Starting today, the state-of-the-art Falcon 40B foundation model from Technology\n",
      "    Innovation Institute (TII) is available on Amazon SageMaker JumpStart, SageMaker's machine learning (ML) hub\n",
      "    that offers pre-trained models, built-in algorithms, and pre-built solution templates to help you quickly get\n",
      "    started with ML. You can deploy and use this Falcon LLM with a few clicks in SageMaker Studio or\n",
      "    programmatically through the SageMaker Python SDK.\n",
      "    Falcon 40B is a 40-billion-parameter large language model (LLM) available under the Apache 2.0 license that\n",
      "    ranked #1 in Hugging Face Open LLM leaderboard, which tracks, ranks, and evaluates LLMs across multiple\n",
      "    benchmarks to identify top performing models. Since its release in May 2023, Falcon 40B has demonstrated\n",
      "    exceptional performance without specialized fine-tuning. To make it easier for customers to access this\n",
      "    state-of-the-art model, AWS has made Falcon 40B available to customers via Amazon SageMaker JumpStart.\n",
      "    Now customers can quickly and easily deploy their own Falcon 40B model and customize it to fit their specific\n",
      "    needs for applications such as translation, question answering, and summarizing information.\n",
      "    Falcon 40B are generally available today through Amazon SageMaker JumpStart in US East (Ohio),\n",
      "    US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), Asia Pacific (Seoul), Asia Pacific (Mumbai),\n",
      "    Europe (London), Europe (Frankfurt), Europe (Ireland), and Canada (Central),\n",
      "    with availability in additional AWS Regions coming soon. To learn how to use this new feature,\n",
      "    please see SageMaker JumpStart documentation, the Introduction to SageMaker JumpStart ‚Äì\n",
      "    Text Generation with Falcon LLMs example notebook, and the blog Technology Innovation Institute trainsthe\n",
      "    state-of-the-art Falcon LLM 40B foundation model on Amazon SageMaker. Summarize the article above: [/INST]\n",
      "\u001b[1m Output:\u001b[0m   Amazon SageMaker JumpStart has added the state-of-the-art Falcon 40B foundation model from Technology Innovation Institute (TII) to its machine learning (ML) hub. This 40-billion-parameter large language model (LLM) is available under the Apache 2.0 license and has ranked #1 on the Hugging Face Open LLM leaderboard. Customers can now quickly and easily deploy and customize the Falcon 40B model for applications such as translation, question answering, and summarizing information through Amazon SageMaker JumpStart. The model is generally available in US East (Ohio), US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), Asia Pacific (Seoul), Asia Pacific (Mumbai), Europe (London), Europe (Frankfurt), Europe (Ireland), and Canada (Central), with\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "CPU times: user 6.75 ms, sys: 3.48 ms, total: 10.2 ms\n",
      "Wall time: 57.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for payload in additional_examples:\n",
    "    query_endpoint(payload)\n",
    "    print(\"---------------------------------------------------------------------------\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e062d29",
   "metadata": {},
   "source": [
    "## Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24cc5560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b575c86b-ffae-45f1-a212-5159d137aadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-25 01:06:44.946718\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37b40d8-1bd7-4302-982b-8a8e744cdc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.g5.8xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
