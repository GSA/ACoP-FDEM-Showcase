{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8af3794b",
   "metadata": {},
   "source": [
    "# Run Llama 2 Models in SageMaker JumpStart"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446b1b24",
   "metadata": {},
   "source": [
    "---\n",
    "In this demo notebook, we demonstrate how to use the SageMaker Python SDK to deploy a JumpStart model for Text Generation using the Llama 2 fine-tuned model optimized for dialogue use cases.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35642ab2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32b2da2e-cb9c-44a8-9fd4-759e13ac459c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-25 11:55:55.759186\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b55e677-3429-4668-b100-bd63d2a4c401",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d458cf0-02e2-4066-927b-25fa5ef2a07e",
   "metadata": {},
   "source": [
    "***\n",
    "You can continue with the default model or choose a different model: this notebook will run with the following model IDs :\n",
    "- `meta-textgeneration-llama-2-7b-f`\n",
    "- `meta-textgeneration-llama-2-13b-f`\n",
    "- `meta-textgeneration-llama-2-70b-f`\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62fd54f6",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdOnly"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id = \"meta-textgeneration-llama-2-7b-f\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "084aa53c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_version = \"3.*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eef0dd",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "\n",
    "***\n",
    "You can now deploy the model using SageMaker JumpStart. For successful deployment, you must manually change the `accept_eula` argument in the model's deploy method to `True`.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e52afae-868d-4736-881f-7180f393003a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /root/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using vulnerable JumpStart model 'meta-textgeneration-llama-2-7b-f' and version '3.2.0'.\n",
      "Using model 'meta-textgeneration-llama-2-7b-f' with wildcard version identifier '3.*'. You can pin to version '3.2.0' for more stable results. Note that models may have different input/output signatures after a major version upgrade.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.73 s, sys: 4.89 s, total: 6.62 s\n",
      "Wall time: 1.64 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "model = JumpStartModel(model_id=model_id, model_version=model_version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a48f26bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------!CPU times: user 80.9 ms, sys: 4.92 ms, total: 85.8 ms\n",
      "Wall time: 6min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictor = model.deploy(accept_eula=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef7207e-01ba-4ac2-b4a9-c8f6f0e1c498",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invoke the endpoint\n",
    "\n",
    "***\n",
    "### Supported Parameters\n",
    "\n",
    "***\n",
    "This model supports many parameters while performing inference. They include:\n",
    "\n",
    "* **max_length:** Model generates text until the output length (which includes the input context length) reaches `max_length`. If specified, it must be a positive integer.\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches `max_new_tokens`. If specified, it must be a positive integer.\n",
    "* **num_beams:** Number of beams used in the greedy search. If specified, it must be integer greater than or equal to `num_return_sequences`.\n",
    "* **no_repeat_ngram_size:** Model ensures that a sequence of words of `no_repeat_ngram_size` is not repeated in the output sequence. If specified, it must be a positive integer greater than 1.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **early_stopping:** If True, text generation is finished when all beam hypotheses reach the end of sentence token. If specified, it must be boolean.\n",
    "* **do_sample:** If True, sample the next word as per the likelihood. If specified, it must be boolean.\n",
    "* **top_k:** In each step of text generation, sample from only the `top_k` most likely words. If specified, it must be a positive integer.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "* **return_full_text:** If True, input text will be part of the output generated text. If specified, it must be boolean. The default value for it is False.\n",
    "* **stop**: If specified, it must be a list of strings. Text generation stops if any one of the specified strings is generated.\n",
    "\n",
    "We may specify any subset of the parameters mentioned above while invoking an endpoint. Next, we show an example of how to invoke endpoint with these arguments.\n",
    "\n",
    "**NOTE**: If `max_new_tokens` is not defined, the model may generate up to the maximum total tokens allowed, which is 4K for these models. This may result in endpoint query timeout errors, so it is recommended to set `max_new_tokens` when possible. For 7B, 13B, and 70B models, we recommend to set `max_new_tokens` no greater than 1500, 1000, and 500 respectively, while keeping the total number of tokens less than 4K.\n",
    "\n",
    "**NOTE**: This model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and alternating (u/a/u/a/u...).\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e845ae",
   "metadata": {},
   "source": [
    "### Example prompts\n",
    "***\n",
    "The examples in this section demonstrate how to perform text generation with conversational dialog as prompt inputs. Example payloads are retrieved programmatically from the `JumpStartModel` object.\n",
    "\n",
    "Input messages for Llama-2 chat models should exhibit the following format. The model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and alternating (u/a/u/a/u...). The last message must be from 'user'. A simple user prompt may look like the following:\n",
    "```\n",
    "<s>[INST] {user_prompt} [/INST]\n",
    "```\n",
    "You may also add a system prompt with the following syntax:\n",
    "```\n",
    "<s>[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "\n",
    "{user_prompt} [/INST]\n",
    "```\n",
    "Finally, you can have a conversational interaction with the model by including all previous user prompts and assistant responses in the input:\n",
    "```\n",
    "<s>[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "\n",
    "{user_prompt_1} [/INST] {assistant_response_1} </s><s>[INST] {user_prompt_1} [/INST]\n",
    "```\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2255b012-0f95-427c-bf1f-9104f651b5e7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 261 µs, sys: 0 ns, total: 261 µs\n",
      "Wall time: 265 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "example_payloads = model.retrieve_all_examples()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "952395d9-2254-4fe6-b42c-f4f763c93744",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'inputs': '<s>[INST] what is the recipe of mayonnaise? [/INST] ',\n",
       " 'parameters': {'max_new_tokens': 256,\n",
       "  'top_p': 0.9,\n",
       "  'temperature': 0.6,\n",
       "  'decoder_input_details': True,\n",
       "  'details': True}}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_payloads[0].body"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4fc07501",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input\n",
      " {'inputs': '<s>[INST] what is the recipe of mayonnaise? [/INST] ', 'parameters': {'max_new_tokens': 256, 'top_p': 0.9, 'temperature': 0.6, 'decoder_input_details': True, 'details': True}} \n",
      "\n",
      "Output\n",
      " Mayonnaise is a thick, creamy condiment made from a mixture of egg yolks, oil, vinegar or lemon juice, and seasonings. Here is a basic recipe for homemade mayonnaise:\n",
      "\n",
      "Ingredients:\n",
      "\n",
      "* 2 large egg yolks\n",
      "* 1/2 cup (120 ml) neutral-tasting oil, such as canola or grapeseed\n",
      "* 1 tablespoon (15 ml) vinegar or lemon juice\n",
      "* 1/2 teaspoon salt\n",
      "* 1/4 teaspoon cayenne pepper (optional, for some heat)\n",
      "\n",
      "Instructions:\n",
      "\n",
      "1. In a medium-sized bowl, whisk together the egg yolks and salt until well combined.\n",
      "2. Slowly pour in the oil while continuously whisking the mixture. You can do this by hand with a whisk or use an electric mixer on low speed.\n",
      "3. Continue whisking until the mixture thickens and emulsifies, which should take about 5-7 minutes. You will know it' \n",
      "\n",
      "===============\n",
      "\n",
      "Input\n",
      " {'inputs': \"<s>[INST] I am going to Paris, what should I see? [/INST] Paris, the capital of France, is known for its stunning architecture, art museums, historical landmarks, and romantic atmosphere. Here are some of the top attractions to see in Paris:\\n\\n1. The Eiffel Tower: The iconic Eiffel Tower is one of the most recognizable landmarks in the world and offers breathtaking views of the city.\\n2. The Louvre Museum: The Louvre is one of the world's largest and most famous museums, housing an impressive collection of art and artifacts, including the Mona Lisa.\\n3. Notre-Dame Cathedral: This beautiful cathedral is one of the most famous landmarks in Paris and is known for its Gothic architecture and stunning stained glass windows.\\n\\nThese are just a few of the many attractions that Paris has to offer. With so much to see and do, it's no wonder that Paris is one of the most popular tourist destinations in the world.</s><s>[INST] What is so great about #1? [/INST] \", 'parameters': {'max_new_tokens': 256, 'top_p': 0.9, 'temperature': 0.6}} \n",
      "\n",
      "Output\n",
      " The Eiffel Tower is considered one of the most iconic and recognizable landmarks in the world, and there are several reasons why it's so great:\n",
      "\n",
      "1. Unique Design: The Eiffel Tower's design is unlike any other structure in the world. Its lattice-like framework and intricate details make it a true engineering marvel.\n",
      "2. Historical Significance: The Eiffel Tower was built for the 1889 World's Fair in Paris, held to celebrate the 100th anniversary of the French Revolution. It was originally intended to be a temporary structure, but it has become a permanent fixture in the Parisian skyline.\n",
      "3. Breathtaking Views: The Eiffel Tower offers panoramic views of the city of Paris from its observation decks on the first and second levels. Visitors can see many of the city's famous landmarks, such as the Seine River, the Arc de Triomphe, and the Champs-Élysées.\n",
      "4. Cultural Significance: The Eiffel Tower has become a symbol of Paris and of France itself. It's featured in countless films, books \n",
      "\n",
      "===============\n",
      "\n",
      "Input\n",
      " {'inputs': '<s>[INST] <<SYS>>\\nAlways answer with Haiku\\n<</SYS>>\\n\\nI am going to Paris, what should I see? [/INST] ', 'parameters': {'max_new_tokens': 256, 'top_p': 0.9, 'temperature': 0.6}} \n",
      "\n",
      "Output\n",
      " Eiffel Tower high\n",
      "Love locks on bridges glisten\n",
      "City of romance \n",
      "\n",
      "===============\n",
      "\n",
      "Input\n",
      " {'inputs': '<s>[INST] <<SYS>>\\nAlways answer with emojis\\n<</SYS>>\\n\\nHow to go from Beijing to NY? [/INST] ', 'parameters': {'max_new_tokens': 256, 'top_p': 0.9, 'temperature': 0.6}} \n",
      "\n",
      "Output\n",
      " Here's how to get from Beijing to New York 🛫🗺️:\n",
      "\n",
      "1. Fly from Beijing Capital International Airport (PEK) to John F. Kennedy International Airport (JFK) or LaGuardia Airport (LGA) in New York City.\n",
      "\n",
      "🛬🚀\n",
      "\n",
      "2. Alternatively, you can take a train or bus from Beijing to New York, but this option is less common and may take longer.\n",
      "\n",
      "🚂🚌\n",
      "\n",
      "Let me know if you have any other questions! 🤔 \n",
      "\n",
      "===============\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for payload in example_payloads:\n",
    "    response = predictor.predict(payload.body)\n",
    "    print(\"\\nInput\\n\", payload.body, \"\\n\\nOutput\\n\", response[0][\"generated_text\"], \"\\n\\n===============\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d76d95",
   "metadata": {},
   "source": [
    "***\n",
    "While not used in the previously provided example payloads, you can format your own messages to the Llama-2 model with the following utility function.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5adf9b4-c7e1-4090-aefe-9cae0d096968",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>[INST] <<SYS>>\\nYou are writing strategic objective statements for US government goals. You work for the US Department of Health and Human Services. Each statement should clearly communicate what the agency is trying to accomplish. It should energize Americans who have a stake in the issue to get involved. Rewrite the strategic objective statement at the bottom to make it stronger. Generate three options. Do not use jargon. Write for a 10th-grade reading level or lower. Format the statement so that it is easy to skim and read. Use less than 225 characters.\\n<</SYS>>\\n\\nRewrite: Drive the integration of behavioral health into the healthcare system to strengthen and expand access to mental health and substance use disorder treatment and recovery services for individuals and families. [/INST] '"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict, List\n",
    "\n",
    "\n",
    "def format_messages(messages: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"Format messages for Llama-2 chat models.\n",
    "    \n",
    "    The model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and \n",
    "    alternating (u/a/u/a/u...). The last message must be from 'user'.\n",
    "    \"\"\"\n",
    "    prompt: List[str] = []\n",
    "\n",
    "    if messages[0][\"role\"] == \"system\":\n",
    "        content = \"\".join([\"<<SYS>>\\n\", messages[0][\"content\"], \"\\n<</SYS>>\\n\\n\", messages[1][\"content\"]])\n",
    "        messages = [{\"role\": messages[1][\"role\"], \"content\": content}] + messages[2:]\n",
    "\n",
    "    for user, answer in zip(messages[::2], messages[1::2]):\n",
    "        prompt.extend([\"<s>\", \"[INST] \", (user[\"content\"]).strip(), \" [/INST] \", (answer[\"content\"]).strip(), \"</s>\"])\n",
    "\n",
    "    prompt.extend([\"<s>\", \"[INST] \", (messages[-1][\"content\"]).strip(), \" [/INST] \"])\n",
    "\n",
    "    return \"\".join(prompt)\n",
    "\n",
    "\n",
    "dialog = [\n",
    "    {\"role\": \"system\", \"content\": \"You are writing strategic objective statements for US government goals. You work for the US Department of Health and Human Services. Each statement should clearly communicate what the agency is trying to accomplish. It should energize Americans who have a stake in the issue to get involved. Rewrite the strategic objective statement at the bottom to make it stronger. Generate three options. Do not use jargon. Write for a 10th-grade reading level or lower. Format the statement so that it is easy to skim and read. Use less than 225 characters.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Rewrite: Drive the integration of behavioral health into the healthcare system to strengthen and expand access to mental health and substance use disorder treatment and recovery services for individuals and families.\"\"\"},\n",
    "]\n",
    "\n",
    "prompt = format_messages(dialog)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a705dc5-3f1e-4764-a489-f0a9f5ff1d91",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload= {'inputs': prompt,\n",
    " 'parameters': {'max_new_tokens': 256,\n",
    "  'top_p': 0.9,\n",
    "  'temperature': 0.6,\n",
    "  'decoder_input_details': True,\n",
    "  'details': True}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "af602a42-79f9-4ae7-9247-a6a2d61f767f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Option 1:\n",
      "\"Improve Mental Health and Wellness for All: Integrate Behavioral Health into Healthcare for Better Treatment and Recovery\"\n",
      "\n",
      "Option 2:\n",
      "\"Transform Healthcare to Include Mental Health: Ensure Access to Life-Changing Treatment and Support for Individuals and Families\"\n",
      "\n",
      "Option 3:\n",
      "\"Unlock Mental Health Potential: Connect Behavioral Health with Primary Care for Holistic Health and Wellness\"\n",
      "\n",
      "Each of these options is clear, concise, and easy to understand, while also conveying the importance of integrating behavioral health into the healthcare system. They are written at a 10th-grade reading level or lower, and use language that is accessible to a wide range of audiences.\n"
     ]
    }
   ],
   "source": [
    "response = predictor.predict(payload)\n",
    "print(response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62e23428-4dda-4b31-a160-aa602c4816db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def reformat_user_prompt(user_prompt):\n",
    "    return f\"<s>[INST] {user_prompt} [/INST]\"\n",
    "\n",
    "additional_examples = [\n",
    "    {\n",
    "    \"inputs\": \"Tell me about General Services Administration.\",\n",
    "    \"parameters\": {\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.8,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"stop\": [\"<|endoftext|>\", \"</s>\"]\n",
    "    }\n",
    "},  # tell me about...\n",
    "{\n",
    "    \"inputs\": \"Tell me about Amazon SageMaker.\",\n",
    "    \"parameters\": {\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.8,\n",
    "        \"max_new_tokens\": 1024,\n",
    "        \"stop\": [\"<|endoftext|>\", \"</s>\"]\n",
    "    }\n",
    "}, # summarization\n",
    " {\n",
    "    \"inputs\":\"\"\"Starting today, the state-of-the-art Falcon 40B foundation model from Technology\n",
    "    Innovation Institute (TII) is available on Amazon SageMaker JumpStart, SageMaker's machine learning (ML) hub\n",
    "    that offers pre-trained models, built-in algorithms, and pre-built solution templates to help you quickly get\n",
    "    started with ML. You can deploy and use this Falcon LLM with a few clicks in SageMaker Studio or\n",
    "    programmatically through the SageMaker Python SDK.\n",
    "    Falcon 40B is a 40-billion-parameter large language model (LLM) available under the Apache 2.0 license that\n",
    "    ranked #1 in Hugging Face Open LLM leaderboard, which tracks, ranks, and evaluates LLMs across multiple\n",
    "    benchmarks to identify top performing models. Since its release in May 2023, Falcon 40B has demonstrated\n",
    "    exceptional performance without specialized fine-tuning. To make it easier for customers to access this\n",
    "    state-of-the-art model, AWS has made Falcon 40B available to customers via Amazon SageMaker JumpStart.\n",
    "    Now customers can quickly and easily deploy their own Falcon 40B model and customize it to fit their specific\n",
    "    needs for applications such as translation, question answering, and summarizing information.\n",
    "    Falcon 40B are generally available today through Amazon SageMaker JumpStart in US East (Ohio),\n",
    "    US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), Asia Pacific (Seoul), Asia Pacific (Mumbai),\n",
    "    Europe (London), Europe (Frankfurt), Europe (Ireland), and Canada (Central),\n",
    "    with availability in additional AWS Regions coming soon. To learn how to use this new feature,\n",
    "    please see SageMaker JumpStart documentation, the Introduction to SageMaker JumpStart –\n",
    "    Text Generation with Falcon LLMs example notebook, and the blog Technology Innovation Institute trainsthe\n",
    "    state-of-the-art Falcon LLM 40B foundation model on Amazon SageMaker. Summarize the article above:\"\"\",\n",
    "    \"parameters\":{\n",
    "        \"max_new_tokens\":200\n",
    "        }\n",
    "    },\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "220dda53-a298-42a1-a951-b24cd72288fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def query_endpoint(payload):\n",
    "    \"\"\"Query endpoint and print the response\"\"\"\n",
    "    response = predictor.predict(payload)\n",
    "    print(f\"\\033[1m Input:\\033[0m {payload['inputs']}\")\n",
    "    print(f\"\\033[1m Output:\\033[0m {response[0]['generated_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5b329707-ec03-4140-bb15-1ecd3c3669b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ex in additional_examples:\n",
    "    text = reformat_user_prompt(ex[\"inputs\"])\n",
    "    ex[\"inputs\"] = text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec73cbab-1336-4227-88e5-ae41f152e8dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(len(additional_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "98383b09-b255-4ce8-98fe-9f0f2800f133",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m Input:\u001b[0m <s>[INST] Tell me about General Services Administration. [/INST]\n",
      "\u001b[1m Output:\u001b[0m   The General Services Administration (GSA) is a federal agency in the United States that provides centralized procurement, property management, and other administrative services to the federal government. The GSA was established in 1949 to help federal agencies streamline their operations and reduce costs by providing common services and resources.\n",
      "\n",
      "Here are some of the key functions of the GSA:\n",
      "\n",
      "1. Procurement: The GSA provides a wide range of procurement services to federal agencies, including contract management, acquisition planning, and vendor selection. The GSA also operates the Federal Supply Schedule (FSS), which is a long-term contracting vehicle that allows federal agencies to purchase goods and services from pre-approved vendors at competitive prices.\n",
      "2. Property Management: The GSA is responsible for managing the real estate assets of the federal government, including leasing and managing office space, warehouses, and other facilities. The GSA also provides moving services and helps federal agencies dispose of excess property.\n",
      "3. Information Technology: The GSA provides a range of IT services to federal agencies, including cloud computing, cybersecurity, and data center management. The GSA also operates the Federal Cloud Computing Strategy, which provides federal agencies with access to cloud-based services and tools.\n",
      "4. Telecommunications: The GSA provides telecommunications services to federal agencies, including internet access, voice and data services, and network management.\n",
      "5. Security: The GSA provides security services to federal agencies, including security assessments, threat monitoring, and incident response.\n",
      "6. Facilities Management: The GSA provides facilities management services to federal agencies, including building maintenance, energy management, and sustainability services.\n",
      "7. Travel: The GSA operates the Federal Travel Management System (FTMS), which provides federal employees with a centralized platform for booking travel reservations, managing expenses, and staying compliant with government travel policies.\n",
      "8. Fleet Management: The GSA provides fleet management services to federal agencies, including vehicle procurement, maintenance, and fuel management.\n",
      "9. Grants Management: The GSA provides grants management services to federal agencies, including grants administration, financial management, and reporting.\n",
      "\n",
      "The GSA is led by a Administrator, who is appointed by the President and confirmed by the Senate. The GSA has a number of offices and centers located throughout the United States, including the National Capital Region, Denver, and San Francisco. The GSA also has a number of partnerships with other federal agencies, private sector companies, and non-profit organizations to provide additional services and support to federal agencies.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\u001b[1m Input:\u001b[0m <s>[INST] Tell me about Amazon SageMaker. [/INST]\n",
      "\u001b[1m Output:\u001b[0m   Amazon SageMaker is a cloud-based machine learning platform provided by Amazon Web Services (AWS). It is designed to help data scientists and machine learning practitioners build, train, and deploy machine learning models more quickly and easily, without having to worry about the underlying infrastructure.\n",
      "\n",
      "Here are some key features and capabilities of Amazon SageMaker:\n",
      "\n",
      "1. Automated Machine Learning: Amazon SageMaker provides a range of automated machine learning algorithms that can be used to build, train, and deploy machine learning models. These algorithms can automatically handle tasks such as data preparation, feature engineering, and model selection, making it easier for non-technical users to build machine learning models.\n",
      "2. Support for Multiple Machine Learning Frameworks: Amazon SageMaker supports a range of machine learning frameworks, including TensorFlow, PyTorch, and scikit-learn. This allows data scientists to use the framework they are most comfortable with, while still benefiting from the platform's automated infrastructure and management capabilities.\n",
      "3. Automated Model Training: Amazon SageMaker provides automated tools for training machine learning models. Users can simply select the dataset they want to use, choose the machine learning algorithm they want to use, and the platform will handle the rest, including data preparation, model training, and model tuning.\n",
      "4. Model Evaluation and Hyperparameter Tuning: Amazon SageMaker provides tools for evaluating and tuning machine learning models. Users can use the platform to compare the performance of different models, and to optimize the hyperparameters of a model to improve its performance.\n",
      "5. Deployment: Amazon SageMaker provides tools for deploying machine learning models to a production environment. Users can use the platform to deploy their models to AWS services such as Amazon S3, Amazon EC2, and Amazon Lambda, or to other cloud providers.\n",
      "6. Collaboration and Version Control: Amazon SageMaker provides tools for collaboration and version control. Users can invite colleagues to collaborate on a model, and can track changes to the model over time.\n",
      "7. Integration with Other AWS Services: Amazon SageMaker is integrated with other AWS services, such as AWS Glue, AWS Lambda, and Amazon ECS, allowing users to easily integrate their machine learning models with these services.\n",
      "8. Cost-Effective: Amazon SageMaker is a cost-effective platform, users only pay for the resources they use, and there are no upfront costs or commitments.\n",
      "9. Security and Compliance: Amazon SageMaker is built on top of AWS, which provides a robust security and compliance infrastructure, ensuring that user data is secure and compliant with regulatory requirements.\n",
      "10. Continuous Improvement: Amazon SageMaker is a continuously improving platform, with new features and capabilities being added regularly.\n",
      "\n",
      "Overall, Amazon SageMaker is a powerful platform that can help organizations of all sizes to build, train, and deploy machine learning models more quickly and easily, without having to worry about the underlying infrastructure. It's a great tool for data scientists and machine learning practitioners, as well as for organizations that are just starting to explore the world of machine learning.\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "\u001b[1m Input:\u001b[0m <s>[INST] Starting today, the state-of-the-art Falcon 40B foundation model from Technology\n",
      "    Innovation Institute (TII) is available on Amazon SageMaker JumpStart, SageMaker's machine learning (ML) hub\n",
      "    that offers pre-trained models, built-in algorithms, and pre-built solution templates to help you quickly get\n",
      "    started with ML. You can deploy and use this Falcon LLM with a few clicks in SageMaker Studio or\n",
      "    programmatically through the SageMaker Python SDK.\n",
      "    Falcon 40B is a 40-billion-parameter large language model (LLM) available under the Apache 2.0 license that\n",
      "    ranked #1 in Hugging Face Open LLM leaderboard, which tracks, ranks, and evaluates LLMs across multiple\n",
      "    benchmarks to identify top performing models. Since its release in May 2023, Falcon 40B has demonstrated\n",
      "    exceptional performance without specialized fine-tuning. To make it easier for customers to access this\n",
      "    state-of-the-art model, AWS has made Falcon 40B available to customers via Amazon SageMaker JumpStart.\n",
      "    Now customers can quickly and easily deploy their own Falcon 40B model and customize it to fit their specific\n",
      "    needs for applications such as translation, question answering, and summarizing information.\n",
      "    Falcon 40B are generally available today through Amazon SageMaker JumpStart in US East (Ohio),\n",
      "    US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), Asia Pacific (Seoul), Asia Pacific (Mumbai),\n",
      "    Europe (London), Europe (Frankfurt), Europe (Ireland), and Canada (Central),\n",
      "    with availability in additional AWS Regions coming soon. To learn how to use this new feature,\n",
      "    please see SageMaker JumpStart documentation, the Introduction to SageMaker JumpStart –\n",
      "    Text Generation with Falcon LLMs example notebook, and the blog Technology Innovation Institute trainsthe\n",
      "    state-of-the-art Falcon LLM 40B foundation model on Amazon SageMaker. Summarize the article above: [/INST]\n",
      "\u001b[1m Output:\u001b[0m   Amazon SageMaker JumpStart has added the state-of-the-art Falcon 40B foundation model from Technology Innovation Institute (TII) to its machine learning (ML) hub. This 40-billion-parameter large language model (LLM) is available under the Apache 2.0 license and has ranked #1 on the Hugging Face Open LLM leaderboard. Customers can now quickly and easily deploy and customize the Falcon 40B model for applications such as translation, question answering, and summarizing information through Amazon SageMaker JumpStart. The model is generally available in US East (Ohio), US East (N. Virginia), US West (Oregon), Asia Pacific (Tokyo), Asia Pacific (Seoul), Asia Pacific (Mumbai), Europe (London), Europe (Frankfurt), Europe (Ireland), and Canada (Central), with\n",
      "---------------------------------------------------------------------------\n",
      "\n",
      "\n",
      "CPU times: user 12.8 ms, sys: 1.57 ms, total: 14.3 ms\n",
      "Wall time: 46.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for payload in additional_examples:\n",
    "    query_endpoint(payload)\n",
    "    print(\"---------------------------------------------------------------------------\\n\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e062d29",
   "metadata": {},
   "source": [
    "## Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "24cc5560",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b575c86b-ffae-45f1-a212-5159d137aadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-25 12:03:18.595730\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37b40d8-1bd7-4302-982b-8a8e744cdc6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18465cda-518b-4c0e-9f63-09fe8cd18728",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64b9961-a183-4a09-8a6e-f8a0c89cb9c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.g5.8xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
